<script setup lang="ts">
import * as tf from "@tensorflow/tfjs"
import * as posenet from "@tensorflow-models/posenet"
import * as poseDetection from "@tensorflow-models/pose-detection"
import "@tensorflow/tfjs-backend-webgl"

import bgImage from "@/assets/image/bg.jpg"
import { PoseDetector } from "@tensorflow-models/pose-detection/dist/pose_detector"
import { PoseNet } from "@tensorflow-models/posenet/dist/posenet_model"

const model = poseDetection.SupportedModels.BlazePose
// const model = poseDetection.SupportedModels.MoveNet

const detectorConfig = {
  runtime: "tfjs",
  modelType: "full",
}
let loading = ref(false)
let video: HTMLVideoElement
let canvas: HTMLCanvasElement
let poseNet: PoseNet

onMounted(async () => {
  await tf.setBackend("webgl")
  await init()
})

// æ‰“å¼€æ‘„åƒå¤´
async function init() {
  loading.value = true
  await poseDetection.createDetector(model, detectorConfig)

  video = document.getElementById("local-video") as HTMLVideoElement

  video.srcObject = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: { facingMode: "user" },
  })

  canvas = document.getElementById("canvas") as HTMLCanvasElement
  poseNet = await posenet.load()

  loading.value = false
}

async function identify() {
  const ctx = canvas.getContext("2d")!

  // è¯†åˆ«å›¾åƒä¸­çš„å§¿åŠ¿(å•å§¿æ€)
  const poses = await poseNet.estimatePoses(video, { flipHorizontal: false, decodingMethod: "single-person" })
  console.log("ğŸš€ğŸš€ğŸš€ / detector", poseNet)
  console.log("ğŸš€ğŸš€ğŸš€ / pose", poses)
  const pose = await poseNet.estimateSinglePose(video, { flipHorizontal: false })

  // å°† pose ä¸Šçš„ 17 ä¸ªå…³é”®ç‚¹çš„åæ ‡ä¿¡æ¯å­˜å…¥ pointList
  const pointList = pose.keypoints
  // eslint-disable-next-line no-console
  // console.log('ğŸš€ğŸš€ğŸš€ /  pointList ', pointList)
  // å°†è¿™ 17 ä¸ªå…³é”®ç‚¹çš„åæ ‡ä¿¡æ¯ ç”»åˆ° canvas ä¸Š
  ctx.clearRect(0, 0, canvas.width, canvas.height)
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height)

  // ç”»å‡º 17 ä¸ªå…³é”®ç‚¹
  pointList.forEach(({ position }) => {
    ctx.beginPath()
    ctx.arc(position.x, position.y, 4, 0, 2 * Math.PI)
    ctx.fillStyle = "#f00000"
    ctx.fill()
  })
  // ç”»å‡º 17 ä¸ªå…³é”®ç‚¹ä¹‹é—´çš„è¿çº¿
  const adjacentKeyPoints = posenet.getAdjacentKeyPoints(pose.keypoints, 0.3)
  // console.log('ğŸš€ğŸš€ğŸš€ / adjacentKeyPoints', adjacentKeyPoints)
  adjacentKeyPoints.forEach((point) => {
    drawSegment(toTuple(point[0].position), toTuple(point[1].position), "aqua", 1, ctx)
  })

  // requestAnimationFrame(identify)
  setTimeout(identify, 500)
}

// è½¬æˆå…ƒç»„
function toTuple({ y, x }: { y: number; x: number }) {
  return [y, x]
}
// ç”»çº¿æ®µ
function drawSegment([ay, ax]: number[], [by, bx]: number[], color: string, scale: number, ctx: CanvasRenderingContext2D) {
  ctx.beginPath()
  ctx.moveTo(ax * scale, ay * scale)
  ctx.lineTo(bx * scale, by * scale)
  ctx.lineWidth = 4
  ctx.strokeStyle = color
  ctx.stroke()
}
</script>
<template>
  <div v-loading="loading" class="page-container">
    <el-button type="primary" size="default" @click="identify">è¯†åˆ«</el-button>
    <div>
      <video id="local-video" width="360" height="270" autoplay muted></video>

      <canvas id="local-canvas" width="360" height="270"></canvas>
      <canvas id="canvas" width="360" height="270"></canvas>

      <img id="img" :src="bgImage" width="400" alt="" />

      <video id="video" playsinline style="transform: scaleX(-1); visibility: hidden; width: auto; height: auto"></video>
      <canvas id="output"></canvas>
    </div>
  </div>
</template>

<style lang="scss" scoped>
#img,
#local-video,
#canvas {
  width: 360px;
  height: 270px;
  object-fit: fill;
}
</style>
